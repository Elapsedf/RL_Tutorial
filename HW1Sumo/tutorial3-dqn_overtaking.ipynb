{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3088203e-a77b-4eef-a994-9eb4e97f51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于SUMO搭建简单的超车环境\n",
    "\"\"\"\n",
    "此python文件基于SUMO搭建简单的超车环境\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces.box import Box\n",
    "\n",
    "import sumolib\n",
    "import traci\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleHighwayDriving(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self, render_mode, label=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # sumo\n",
    "        self.label = label\n",
    "        self.sumo_config = \"envs/cfg/freeway.sumo.cfg\"\n",
    "        self.arguments = [\"--lanechange.duration\", \"0.85\", \"--quit-on-end\"]\n",
    "        add_args = [\"--delay\", \"100\", \"-c\"] # 设置仿真延迟，范围0~1000\n",
    "        self.arguments.extend(add_args)\n",
    "        self.sumo_cmd = [sumolib.checkBinary('sumo')]\n",
    "        self.arguments.append(self.sumo_config)\n",
    "        self.already_running = False\n",
    "\n",
    "        # env\n",
    "        self._target_location = 2100\n",
    "        self.init_speed = 5\n",
    "        self.single_step = 1  # 1step for 1s simulation\n",
    "        self.lane_counts = 1\n",
    "        self.control_id = 'controled_0'\n",
    "\n",
    "        # reward \n",
    "        self.w_speed = 1\n",
    "        self.w_p_time = 0.2\n",
    "        self.w_p_crash = 100\n",
    "\n",
    "        #左转、右转、车道保持\n",
    "        self.n_actions = 3\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "\n",
    "        # 左前、左后、右前、右后、正前方车辆\n",
    "        self.surrounding_num = 5  \n",
    "        F = 3  # diff_speed, diff_x_pos, diff_y_pos\n",
    "        self.observation_space = Box(low=-np.inf,\n",
    "                                     high=np.inf,\n",
    "                                     shape=(self.surrounding_num * F, ),\n",
    "                                     dtype=np.float64)\n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata[\n",
    "            \"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def add_vehicles(self):\n",
    "\n",
    "        # RL 控制车辆\n",
    "        traci.vehicle.add(self.control_id,\n",
    "                          \"route\",\n",
    "                          departPos=0,\n",
    "                          departSpeed=self.init_speed,\n",
    "                          departLane=0,\n",
    "                          typeID='CarB')\n",
    "        traci.vehicle.setLaneChangeMode(self.control_id, 0)\n",
    "\n",
    "        # 其他车辆\n",
    "        traci.vehicle.add('veh_0',\n",
    "                          \"route\",\n",
    "                          departPos=20,\n",
    "                          departSpeed=self.init_speed,\n",
    "                          departLane=0,\n",
    "                          typeID='CarA')\n",
    "        traci.vehicle.setLaneChangeMode('veh_0', 0)   # 禁止车辆的换道\n",
    "        traci.vehicle.setSpeedMode('veh_0', 0) # 禁用的车辆加减速\n",
    "\n",
    "    def _get_obs(self):\n",
    "        surrounding_vehs = []\n",
    "        current_state = []\n",
    "        speed_ego = traci.vehicle.getSpeed(self.control_id)\n",
    "        x_ego, y_ego = traci.vehicle.getPosition(self.control_id)\n",
    "\n",
    "        modes = [\n",
    "            0b000,\n",
    "            0b001,\n",
    "            0b011,\n",
    "            0b010,\n",
    "        ]  #左前、左右、右前、右后车辆\n",
    "        for mode in modes:\n",
    "            veh = traci.vehicle.getNeighbors(self.control_id, mode=mode)\n",
    "            if veh != ():\n",
    "                surrounding_vehs.append(veh[0][0])\n",
    "            else:\n",
    "                surrounding_vehs.append('')\n",
    "        header = traci.vehicle.getLeader(self.control_id)\n",
    "        if not header is None:  # 前车\n",
    "            surrounding_vehs.append(header[0])\n",
    "        else:\n",
    "            surrounding_vehs.append('')\n",
    "        for veh in surrounding_vehs:\n",
    "            if veh == '':\n",
    "                x_diff = 0\n",
    "                y_diff = 0\n",
    "                speed_diff = 0\n",
    "            else:\n",
    "                speed = traci.vehicle.getSpeed(veh)\n",
    "                x, y = traci.vehicle.getPosition(veh)\n",
    "                speed_diff = abs(speed - speed_ego)\n",
    "                x_diff = abs(x - x_ego)\n",
    "                y_diff = abs(y - y_ego)\n",
    "            current_state.append(x_diff)\n",
    "            current_state.append(y_diff)\n",
    "            current_state.append(speed_diff)\n",
    "        return np.array(current_state)\n",
    "\n",
    "    def _get_info(self, **kwargs):\n",
    "        crash = True if len(kwargs['crash_ids']) > 0 else False\n",
    "        return {\n",
    "            \"simulation step\": self.count,\n",
    "            \"crash\": crash,\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        if not self.already_running:\n",
    "\n",
    "            # 是否以可视化方式启动sumo\n",
    "            if self.render_mode == \"human\":\n",
    "                print(\"Creating a sumo-gui.\")\n",
    "                self.sumo_cmd = [sumolib.checkBinary('sumo-gui')] \n",
    "            else:\n",
    "                print(\"No gui will display.\")\n",
    "            self.sumo_cmd.extend(self.arguments)\n",
    "\n",
    "            traci.start(self.sumo_cmd)\n",
    "            self.already_running = True\n",
    "        else:\n",
    "            traci.load(self.arguments)\n",
    "\n",
    "        self.count = 0\n",
    "\n",
    "        self.add_vehicles()\n",
    "\n",
    "        self.count += self.single_step\n",
    "        traci.simulationStep(self.count) # 仿真进行到count秒\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            traci.gui.trackVehicle(\"View #0\", self.control_id)\n",
    "            traci.gui.setZoom(\"View #0\", 1000)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    # 将强化学习模型输出作用于控制车辆\n",
    "    def _apply_rl_action(self, action):\n",
    "        ego_lane = traci.vehicle.getLaneIndex(self.control_id)\n",
    "        if action == 1:\n",
    "            target_lane = min(self.lane_counts, ego_lane + 1)\n",
    "        elif action == 2:\n",
    "            target_lane = max(0, ego_lane - 1)\n",
    "        else:\n",
    "            target_lane = ego_lane\n",
    "\n",
    "        traci.vehicle.changeLane(self.control_id, target_lane, duration=0) # 立刻换道\n",
    "\n",
    "    def _is_done(self):\n",
    "        # 碰撞或者超车完成\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        crash_ids = traci.simulation.getCollidingVehiclesIDList()\n",
    "\n",
    "        pos = traci.vehicle.getPosition(self.control_id)[0]\n",
    "\n",
    "        if pos >= self._target_location:\n",
    "            terminated = True\n",
    "            # print(\"{0} success!\".format(self.control_id))\n",
    "        if self.control_id in crash_ids:\n",
    "            terminated = True\n",
    "            # print('crashing!!! ')\n",
    "\n",
    "        return terminated,truncated, crash_ids\n",
    "\n",
    "    def _get_reward(self, **kwargs):\n",
    "\n",
    "        # 速度奖励、碰撞惩罚、时间惩罚\n",
    "        unit = 1\n",
    "\n",
    "        speed_reward = traci.vehicle.getSpeed(self.control_id)\n",
    "\n",
    "        time = traci.vehicle.getDeparture(self.control_id)\n",
    "        time_penalty = np.array(traci.simulation.getTime() - time)\n",
    "\n",
    "        total_crash_penalty = len(kwargs['crash_ids']) * unit\n",
    "\n",
    "        reward = self.w_speed * speed_reward - self.w_p_time * time_penalty - self.w_p_crash * total_crash_penalty\n",
    "        return np.array(reward)\n",
    "\n",
    "    # 推进一次仿真步\n",
    "    def step(self, action):\n",
    "\n",
    "        self._apply_rl_action(action)\n",
    "        self.count += self.single_step\n",
    "        traci.simulationStep(self.count)\n",
    "\n",
    "        terminated,truncated, crash_ids = self._is_done()\n",
    "        reward = self._get_reward(crash_ids=crash_ids)\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info(crash_ids=crash_ids)\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def close(self):\n",
    "        traci.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2f339a-b2f6-480e-8f17-6554147c83f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\rl_tutorial\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gui will display.\n",
      "\n",
      "\n",
      "85.64050102233887\n",
      "0.0\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "training\n",
      "Logging to ./log/dqn_highway_tensorboard\\DQN_2\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | 825      |\n",
      "|    exploration_rate | 0.575    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 270      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 447      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.37     |\n",
      "|    n_updates        | 86       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.4     |\n",
      "|    ep_rew_mean      | 776      |\n",
      "|    exploration_rate | 0.29     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 747      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.07     |\n",
      "|    n_updates        | 161      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.6     |\n",
      "|    ep_rew_mean      | 787      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1075     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.55     |\n",
      "|    n_updates        | 243      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.4     |\n",
      "|    ep_rew_mean      | 791      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 1399     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.57     |\n",
      "|    n_updates        | 324      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.3     |\n",
      "|    ep_rew_mean      | 792      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 1726     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.7      |\n",
      "|    n_updates        | 406      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89       |\n",
      "|    ep_rew_mean      | 833      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 2135     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.94     |\n",
      "|    n_updates        | 508      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-----------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m evaluate_policy(model,\n\u001b[0;32m     26\u001b[0m                                           env,\n\u001b[0;32m     27\u001b[0m                                           n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     28\u001b[0m                                           deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-----------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\rl_tutorial\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\rl_tutorial\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\rl_tutorial\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:566\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m--> 566\u001b[0m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_locals\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Only stop training if return value is False, not when it is None.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mon_step():\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\rl_tutorial\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:134\u001b[0m, in \u001b[0;36mBaseCallback.update_locals\u001b[1;34m(self, locals_)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_locals\u001b[39m(\u001b[38;5;28mself\u001b[39m, locals_: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Update the references to the local variables.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    :param locals_: the local variables during rollout collection\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocals_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_child_locals(locals_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# env = SimpleHighwayDriving(render_mode='human')\n",
    "env = SimpleHighwayDriving(render_mode=None)\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1,tensorboard_log=\"./log/dqn_highway_tensorboard\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model,\n",
    "                                          env,\n",
    "                                          n_eval_episodes=10,\n",
    "                                          deterministic=False)\n",
    "print('\\n')\n",
    "print(mean_reward)\n",
    "print(std_reward)\n",
    "print('\\n')\n",
    "print('-----------------------------------')\n",
    "print('training')\n",
    "model.learn(total_timesteps=1e4)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model,\n",
    "                                          env,\n",
    "                                          n_eval_episodes=10,\n",
    "                                          deterministic=True)\n",
    "print('-----------------------------------')\n",
    "print('\\n')\n",
    "print(mean_reward)\n",
    "print(std_reward)\n",
    "print('\\n')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
