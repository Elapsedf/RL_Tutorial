{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3088203e-a77b-4eef-a994-9eb4e97f51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于SUMO搭建简单的超车环境\n",
    "\"\"\"\n",
    "此python文件基于SUMO搭建简单的超车环境\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces.box import Box\n",
    "\n",
    "import sumolib\n",
    "import traci\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleHighwayDriving(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self, render_mode, label=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # sumo\n",
    "        self.label = label\n",
    "        self.sumo_config = \"envs/cfg/freeway.sumo.cfg\"\n",
    "        self.arguments = [\"--lanechange.duration\", \"0.85\", \"--quit-on-end\"]\n",
    "        add_args = [\"--delay\", \"100\", \"-c\"] # 设置仿真延迟，范围0~1000\n",
    "        self.arguments.extend(add_args)\n",
    "        self.sumo_cmd = [sumolib.checkBinary('sumo')]\n",
    "        self.arguments.append(self.sumo_config)\n",
    "        self.already_running = False\n",
    "\n",
    "        # env\n",
    "        self._target_location = 2100\n",
    "        self.init_speed = 5\n",
    "        self.single_step = 1  # 1step for 1s simulation\n",
    "        self.lane_counts = 1\n",
    "        self.control_id = 'controled_0'\n",
    "\n",
    "        # reward \n",
    "        self.w_speed = 1\n",
    "        self.w_p_time = 0.2\n",
    "        self.w_p_crash = 100\n",
    "\n",
    "        #左转、右转、车道保持\n",
    "        self.n_actions = 3\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "\n",
    "        # 左前、左后、右前、右后、正前方车辆\n",
    "        self.surrounding_num = 5  \n",
    "        F = 3  # diff_speed, diff_x_pos, diff_y_pos\n",
    "        self.observation_space = Box(low=-np.inf,\n",
    "                                     high=np.inf,\n",
    "                                     shape=(self.surrounding_num * F, ),\n",
    "                                     dtype=np.float64)\n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata[\n",
    "            \"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def add_vehicles(self):\n",
    "\n",
    "        # RL 控制车辆\n",
    "        traci.vehicle.add(self.control_id,\n",
    "                          \"route\",\n",
    "                          departPos=0,\n",
    "                          departSpeed=self.init_speed,\n",
    "                          departLane=0,\n",
    "                          typeID='CarB')\n",
    "        traci.vehicle.setLaneChangeMode(self.control_id, 0)\n",
    "\n",
    "        # 其他车辆\n",
    "        traci.vehicle.add('veh_0',\n",
    "                          \"route\",\n",
    "                          departPos=20,\n",
    "                          departSpeed=self.init_speed,\n",
    "                          departLane=0,\n",
    "                          typeID='CarA')\n",
    "        traci.vehicle.setLaneChangeMode('veh_0', 0)   # 禁止车辆的换道\n",
    "        traci.vehicle.setSpeedMode('veh_0', 0) # 禁用的车辆加减速\n",
    "\n",
    "    def _get_obs(self):\n",
    "        surrounding_vehs = []\n",
    "        current_state = []\n",
    "        speed_ego = traci.vehicle.getSpeed(self.control_id)\n",
    "        x_ego, y_ego = traci.vehicle.getPosition(self.control_id)\n",
    "\n",
    "        modes = [\n",
    "            0b000,\n",
    "            0b001,\n",
    "            0b011,\n",
    "            0b010,\n",
    "        ]  #左前、左右、右前、右后车辆\n",
    "        for mode in modes:\n",
    "            veh = traci.vehicle.getNeighbors(self.control_id, mode=mode)\n",
    "            if veh != ():\n",
    "                surrounding_vehs.append(veh[0][0])\n",
    "            else:\n",
    "                surrounding_vehs.append('')\n",
    "        header = traci.vehicle.getLeader(self.control_id)\n",
    "        if not header is None:  # 前车\n",
    "            surrounding_vehs.append(header[0])\n",
    "        else:\n",
    "            surrounding_vehs.append('')\n",
    "        for veh in surrounding_vehs:\n",
    "            if veh == '':\n",
    "                x_diff = 0\n",
    "                y_diff = 0\n",
    "                speed_diff = 0\n",
    "            else:\n",
    "                speed = traci.vehicle.getSpeed(veh)\n",
    "                x, y = traci.vehicle.getPosition(veh)\n",
    "                speed_diff = abs(speed - speed_ego)\n",
    "                x_diff = abs(x - x_ego)\n",
    "                y_diff = abs(y - y_ego)\n",
    "            current_state.append(x_diff)\n",
    "            current_state.append(y_diff)\n",
    "            current_state.append(speed_diff)\n",
    "        return np.array(current_state)\n",
    "\n",
    "    def _get_info(self, **kwargs):\n",
    "        crash = True if len(kwargs['crash_ids']) > 0 else False\n",
    "        return {\n",
    "            \"simulation step\": self.count,\n",
    "            \"crash\": crash,\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        if not self.already_running:\n",
    "\n",
    "            # 是否以可视化方式启动sumo\n",
    "            if self.render_mode == \"human\":\n",
    "                print(\"Creating a sumo-gui.\")\n",
    "                self.sumo_cmd = [sumolib.checkBinary('sumo-gui')] \n",
    "            else:\n",
    "                print(\"No gui will display.\")\n",
    "            self.sumo_cmd.extend(self.arguments)\n",
    "\n",
    "            traci.start(self.sumo_cmd)\n",
    "            self.already_running = True\n",
    "        else:\n",
    "            traci.load(self.arguments)\n",
    "\n",
    "        self.count = 0\n",
    "\n",
    "        self.add_vehicles()\n",
    "\n",
    "        self.count += self.single_step\n",
    "        traci.simulationStep(self.count) # 仿真进行到count秒\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            traci.gui.trackVehicle(\"View #0\", self.control_id)\n",
    "            traci.gui.setZoom(\"View #0\", 1000)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    # 将强化学习模型输出作用于控制车辆\n",
    "    def _apply_rl_action(self, action):\n",
    "        ego_lane = traci.vehicle.getLaneIndex(self.control_id)\n",
    "        if action == 1:\n",
    "            target_lane = min(self.lane_counts, ego_lane + 1)\n",
    "        elif action == 2:\n",
    "            target_lane = max(0, ego_lane - 1)\n",
    "        else:\n",
    "            target_lane = ego_lane\n",
    "\n",
    "        traci.vehicle.changeLane(self.control_id, target_lane, duration=0) # 立刻换道\n",
    "\n",
    "    def _is_done(self):\n",
    "        # 碰撞或者超车完成\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        crash_ids = traci.simulation.getCollidingVehiclesIDList()\n",
    "\n",
    "        pos = traci.vehicle.getPosition(self.control_id)[0]\n",
    "\n",
    "        if pos >= self._target_location:\n",
    "            terminated = True\n",
    "            # print(\"{0} success!\".format(self.control_id))\n",
    "        if self.control_id in crash_ids:\n",
    "            terminated = True\n",
    "            # print('crashing!!! ')\n",
    "\n",
    "        return terminated,truncated, crash_ids\n",
    "\n",
    "    def _get_reward(self, **kwargs):\n",
    "\n",
    "        # 速度奖励、碰撞惩罚、时间惩罚\n",
    "        unit = 1\n",
    "\n",
    "        speed_reward = traci.vehicle.getSpeed(self.control_id)\n",
    "\n",
    "        time = traci.vehicle.getDeparture(self.control_id)\n",
    "        time_penalty = np.array(traci.simulation.getTime() - time)\n",
    "\n",
    "        total_crash_penalty = len(kwargs['crash_ids']) * unit\n",
    "\n",
    "        reward = self.w_speed * speed_reward - self.w_p_time * time_penalty - self.w_p_crash * total_crash_penalty\n",
    "        return np.array(reward)\n",
    "\n",
    "    # 推进一次仿真步\n",
    "    def step(self, action):\n",
    "\n",
    "        self._apply_rl_action(action)\n",
    "        self.count += self.single_step\n",
    "        traci.simulationStep(self.count)\n",
    "\n",
    "        terminated,truncated, crash_ids = self._is_done()\n",
    "        reward = self._get_reward(crash_ids=crash_ids)\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info(crash_ids=crash_ids)\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def close(self):\n",
    "        traci.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6554f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加环境的复杂度\n",
    "\n",
    "# class HighwayDriving(SimpleHighwayDriving):\n",
    "#     def __init__(self, render_mode, label=None) -> None:\n",
    "#         super().__init__(render_mode, label)\n",
    "#         # put your code here\n",
    "    \n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         super().reset(seed, options)\n",
    "#         # put your code here\n",
    "#         pass\n",
    "    \n",
    "#     def add_vehicles(self):\n",
    "#         pass\n",
    "import random\n",
    "class HighwayDriving(SimpleHighwayDriving):\n",
    "    def __init__(self, render_mode, label=None, num_vehicles=5, vehicle_types=None) -> None:\n",
    "        super().__init__(render_mode, label)\n",
    "        self.num_vehicles = num_vehicles\n",
    "        self.vehicle_types = vehicle_types if vehicle_types is not None else ['CarA', 'CarB', 'CarC']\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed, options)\n",
    "        if not self.already_running:\n",
    "\n",
    "            # 是否以可视化方式启动sumo\n",
    "            if self.render_mode == \"human\":\n",
    "                print(\"Creating a sumo-gui.\")\n",
    "                self.sumo_cmd = [sumolib.checkBinary('sumo-gui')] \n",
    "            else:\n",
    "                print(\"No gui will display.\")\n",
    "            self.sumo_cmd.extend(self.arguments)\n",
    "\n",
    "            traci.start(self.sumo_cmd)\n",
    "            self.already_running = True\n",
    "        else:\n",
    "            traci.load(self.arguments)\n",
    "\n",
    "        self.count = 0\n",
    "\n",
    "        self.add_vehicles()\n",
    "\n",
    "        self.count += self.single_step\n",
    "        traci.simulationStep(self.count) # 仿真进行到count秒\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            traci.gui.trackVehicle(\"View #0\", self.control_id)\n",
    "            traci.gui.setZoom(\"View #0\", 1000)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def add_vehicles(self):\n",
    "        # RL 控制车辆\n",
    "        traci.vehicle.add(self.control_id,\n",
    "                          \"route\",\n",
    "                          departPos=0,\n",
    "                          departSpeed=self.init_speed,\n",
    "                          departLane=0,\n",
    "                          typeID='CarB')\n",
    "        traci.vehicle.setLaneChangeMode(self.control_id, 0)\n",
    "        for i in range(self.num_vehicles):\n",
    "            veh_id = f'veh_{i}'\n",
    "            vehicle_type = random.choice(self.vehicle_types)\n",
    "            depart_pos = random.uniform(10, 50)  # 随机选择车辆出发位置\n",
    "            depart_speed = random.uniform(5, 15)  # 随机选择车辆出发速度\n",
    "            depart_lane = random.randint(0, self.lane_counts - 1)  # 随机选择车辆出发车道\n",
    "            traci.vehicle.add(veh_id,\n",
    "                              \"route\",\n",
    "                              departPos=depart_pos,\n",
    "                              departSpeed=depart_speed,\n",
    "                              departLane=depart_lane,\n",
    "                              typeID=vehicle_type)\n",
    "            traci.vehicle.setLaneChangeMode(veh_id, 0)  # 禁止车辆的换道\n",
    "            traci.vehicle.setSpeedMode(veh_id, 0)  # 禁用的车辆加减速\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806e95b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 基于SB3 改进DQN模型(Double-DQN)\n",
    "# 参考链接： https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/dqn_sb3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7f5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch as th\n",
    "import pdb\n",
    "from stable_baselines3 import DQN\n",
    "class DoubleDQN(DQN):\n",
    "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update learning rate according to schedule\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "\n",
    "        losses = []\n",
    "        for _ in range(gradient_steps):\n",
    "            ### YOUR CODE HERE\n",
    "            # Sample replay buffer\n",
    "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n",
    "\n",
    "            # Do not backpropagate gradient to the target network\n",
    "            with th.no_grad():\n",
    "                # Compute the next Q-values using the target network\n",
    "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
    "                # Decouple action selection from value estimation\n",
    "                # Compute q-values for the next observation using the online q net\n",
    "                next_q_values_online = self.q_net(replay_data.next_observations)\n",
    "                # pdb.set_trace()\n",
    "                # Select action with online network\n",
    "                next_actions_online = next_q_values_online.max(dim=1).indices\n",
    "                # Estimate the q-values for the selected actions using target q network\n",
    "                row_indices = np.arange(len(next_actions_online))\n",
    "                next_q_values = next_q_values[row_indices, next_actions_online]\n",
    "                # Avoid potential broadcast issue\n",
    "                next_q_values = next_q_values.reshape(-1, 1)\n",
    "                # 1-step TD target\n",
    "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
    "\n",
    "            # Get current Q-values estimates\n",
    "            current_q_values = self.q_net(replay_data.observations)\n",
    "\n",
    "            # Retrieve the q-values for the actions from the replay buffer\n",
    "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
    "\n",
    "            # Check the shape\n",
    "            assert current_q_values.shape == target_q_values.shape\n",
    "\n",
    "            # Compute loss (L2 or Huber loss)\n",
    "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "            # losses.append(loss.item())\n",
    "\n",
    "            ### END OF YOUR CODE\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Optimize the q-network\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradient norm\n",
    "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "            self.policy.optimizer.step()\n",
    "\n",
    "        # Increase update counter\n",
    "        self._n_updates += gradient_steps\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/loss\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2f339a-b2f6-480e-8f17-6554147c83f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\rl_tutorial\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gui will display.\n",
      "\n",
      "\n",
      "1045.9894880652428\n",
      "1.0283980516435758\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "Logging to ./dqn_highway_tensorboard/DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.2     |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.893    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 211      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 113      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12.7     |\n",
      "|    n_updates        | 3        |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.8     |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.743    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 270      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.04     |\n",
      "|    n_updates        | 42       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 27.5     |\n",
      "|    exploration_rate | 0.72     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 168      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 295      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 13.2     |\n",
      "|    n_updates        | 48       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 32.8     |\n",
      "|    ep_rew_mean      | 110      |\n",
      "|    exploration_rate | 0.502    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 167      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 524      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.9      |\n",
      "|    n_updates        | 105      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36.6     |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.304    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 166      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 733      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.77     |\n",
      "|    n_updates        | 158      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.6     |\n",
      "|    ep_rew_mean      | 254      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 166      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 1071     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 13.3     |\n",
      "|    n_updates        | 242      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.1     |\n",
      "|    ep_rew_mean      | 336      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 165      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 1403     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.04     |\n",
      "|    n_updates        | 325      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.6     |\n",
      "|    ep_rew_mean      | 424      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 1812     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.73     |\n",
      "|    n_updates        | 427      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 59.3     |\n",
      "|    ep_rew_mean      | 465      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 162      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 2134     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.95     |\n",
      "|    n_updates        | 508      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 63.6     |\n",
      "|    ep_rew_mean      | 522      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 2543     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.21     |\n",
      "|    n_updates        | 610      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 65.4     |\n",
      "|    ep_rew_mean      | 552      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 2878     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.31     |\n",
      "|    n_updates        | 694      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.5     |\n",
      "|    ep_rew_mean      | 592      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 3288     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.31     |\n",
      "|    n_updates        | 796      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71.1     |\n",
      "|    ep_rew_mean      | 627      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 3696     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.67     |\n",
      "|    n_updates        | 898      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 73       |\n",
      "|    ep_rew_mean      | 651      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 4089     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.99     |\n",
      "|    n_updates        | 997      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 75       |\n",
      "|    ep_rew_mean      | 676      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 4500     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.51     |\n",
      "|    n_updates        | 1099     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 76.7     |\n",
      "|    ep_rew_mean      | 698      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 165      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 4911     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.48     |\n",
      "|    n_updates        | 1202     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.2     |\n",
      "|    ep_rew_mean      | 718      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 165      |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 5320     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.86     |\n",
      "|    n_updates        | 1304     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 79.6     |\n",
      "|    ep_rew_mean      | 736      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 165      |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 5728     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.21     |\n",
      "|    n_updates        | 1406     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 80.7     |\n",
      "|    ep_rew_mean      | 752      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 165      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 6136     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.47     |\n",
      "|    n_updates        | 1508     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 81.8     |\n",
      "|    ep_rew_mean      | 767      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 6545     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 1611     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 80.7     |\n",
      "|    ep_rew_mean      | 754      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 6778     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.09     |\n",
      "|    n_updates        | 1669     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 81.4     |\n",
      "|    ep_rew_mean      | 764      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 7163     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.39     |\n",
      "|    n_updates        | 1765     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 82.4     |\n",
      "|    ep_rew_mean      | 773      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 7581     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.85     |\n",
      "|    n_updates        | 1870     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 82.6     |\n",
      "|    ep_rew_mean      | 779      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 7931     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.84     |\n",
      "|    n_updates        | 1957     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 82.6     |\n",
      "|    ep_rew_mean      | 779      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 8256     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.59     |\n",
      "|    n_updates        | 2038     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 84.1     |\n",
      "|    ep_rew_mean      | 803      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 8520     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.96     |\n",
      "|    n_updates        | 2104     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 85.7     |\n",
      "|    ep_rew_mean      | 827      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 8844     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.49     |\n",
      "|    n_updates        | 2185     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.1     |\n",
      "|    ep_rew_mean      | 870      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 162      |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 9207     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.59     |\n",
      "|    n_updates        | 2276     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.2     |\n",
      "|    ep_rew_mean      | 888      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 162      |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 9547     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.91     |\n",
      "|    n_updates        | 2361     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.3     |\n",
      "|    ep_rew_mean      | 917      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 161      |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 9959     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.33     |\n",
      "|    n_updates        | 2464     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "1047.0939821600914\n",
      "0.7007696978235897\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# env = HighwayDriving(render_mode='human')\n",
    "env = HighwayDriving(render_mode=None)\n",
    "\n",
    "model = DoubleDQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./dqn_highway_tensorboard/\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model,\n",
    "                                          env,\n",
    "                                          n_eval_episodes=5,\n",
    "                                          deterministic=False)\n",
    "print('\\n')\n",
    "print(mean_reward)\n",
    "print(std_reward)\n",
    "print('\\n')\n",
    "print('-----------------------------------')\n",
    "\n",
    "model.learn(total_timesteps=1e4)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model,\n",
    "                                          env,\n",
    "                                          n_eval_episodes=5,\n",
    "                                          deterministic=True)\n",
    "print('-----------------------------------')\n",
    "print('\\n')\n",
    "print(mean_reward)\n",
    "print(std_reward)\n",
    "print('\\n')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
